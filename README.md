# Non-Autoregressive Neural Machine Translation papers category

## Mask-Predict and follow-ups:
Mask-Predict: Parallel Decoding of Conditional Masked Language Models
Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation
Non-autoregressive Machine Translation with Disentangled Context Transformer
Infusing Sequential Information into Conditional Masked Translation Model with Self-Review Mechanism
Semi-Autoregressive Training Improves Mask-Predict Decoding
GLAT: Glancing Transformer for Non-Autoregressive Neural Machine Translation
Learning to Rewrite for Non-Autoregressive Neural Machine Translation
MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation\


## Imitation from Autoregressive Neural Machine Translation
Hint-Based Training for Non-Autoregressive Machine Translation
Imitation Learning for Non-Autoregressive Neural Machine Translation
Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation
Fast Structured Decoding for Sequence Models
Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation
Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Translation
Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation
Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation


## Latent-Variable
Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement
Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference Using a Delta Posterior
Non-Autoregressive Translation by Learning Target Categorical Codes


## construct decoder input
Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input
ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation



## Reorder 
Non-autoregressive Transformer by Position Learning
Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation
Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information


## Dynamic edit
Insertion Transformer: Flexible Sequence Generation via Insertion Operations
Levenshtein Transformer


## Others 
Non-Autoregressive Machine Translation with Auxiliary Regularization
Improving Non-autoregressive Neural Machine Translation with Monolingual Data
LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Decoding and Vocabulary Attention
Enriching Non-Autoregressive Transformer with Syntactic and Semantic Structures for Neural Machine Translation
Progressive Multi-Granularity Training for Non-Autoregressive Translation
Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision
Context-Aware Cross-Attention for Non-Autoregressive Translation
Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation
BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining
